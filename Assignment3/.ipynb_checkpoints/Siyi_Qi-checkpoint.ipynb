{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa47aed2",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c775663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d895e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73908cd5",
   "metadata": {},
   "source": [
    "### Exercise 1 Bayesian exercises - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a480e60",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size=3> Answer:</br>1. $logp_{\\theta } (x)$ is the prior probability of the data. Since it does not dependent on z, it is essentially a normalization constant. In addition, $E(a)=a$, if $a$ is a constant. So we could write as line 1.</br></br>\n",
    "2. the product rule is used here. $p(z,x)=p(z|x)p(x)$</br></br>\n",
    "3. Multiply both the numerator and denominator by $q_{\\phi} (z|x)$</br></br>\n",
    "4. The Logarithmic Formula and the properties of Expectation are used here. $log_a(MN)=log_aM+log_aN$ and $E(X+Y)=E(X)+E(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d5b3f",
   "metadata": {},
   "source": [
    "### Exercise 2 Bayesian exercises - Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599f936",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size=3> Answer:</br>1. `x = pm.Data('x', x_data)` should be corrected as `x = pm.Data('x', x_data, mutable=True)` Because in this way, the x is keeping updated to predict y.</br></br>\n",
    "2.`s = pm.Normal(\"sigma\", sigma=0.001)` should be corrected as `s = pm.HalfNormal(\"sigma\", sigma=0.001)` Because the sigma should be a non-negative value.</br></br>\n",
    "3. `likelihood = pm.Normal(\"y\", mu=a*x + b, sigma=s)` should be corrected as `likelihood = pm.Normal(\"y\", mu=a*x + b, sigma=s, observed=y_data)` Because the information of the observed data is needed to define the likelihood function.</br></br>\n",
    "4. `trace = pm.sample(1000, tune=1000, init=None, step=step, cores=2)` should be corrected as `trace = pm.sample(1000, tune=1000, init=None, step=step, chains=2)` The \"cores\" means the number of chains to run in parallel processing tasks. We need to define \"chains\" in the MCMC method here.</br></br>\n",
    "p.s. </br>`y = pm.Data('y_obs', y_data)` is not necessary.</br>\n",
    "</br>\n",
    "`a = pm.Normal(\"slope\", mu=100, sigma=100)\n",
    "b = pm.Normal(\"intercept\", mu=100, sigma=100)\n",
    "s = pm.Normal(\"sigma\", sigma=0.001)`\n",
    "The value of mu and sigma is weird, but it still could work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3fd9f",
   "metadata": {},
   "source": [
    "### Exercise 3 Clustering I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ca0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read in the data\n",
    "dataTrain = np.loadtxt('IDSWeedCropTrain.csv', delimiter=',')\n",
    "dataTest = np.loadtxt('IDSWeedCropTest.csv', delimiter=',')\n",
    "# split input variables and labels\n",
    "XTrain = dataTrain[:, :-1]\n",
    "YTrain = dataTrain[:, -1]\n",
    "XTest = dataTest[:, :-1]\n",
    "YTest = dataTest[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caf089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "startingPoint = np.vstack((XTrain[0,], XTrain[1,]))\n",
    "kmeans = KMeans(2, n_init = 1, init= startingPoint, algorithm = \"full\").fit(XTrain) #set k=2\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adcd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"feature6\"] = XTest[:, 6]\n",
    "df[\"feature7\"] = XTest[:, 7]\n",
    "df[\"feature1\"] = XTest[:, 1]\n",
    "df[\"feature2\"] = XTest[:, 2]\n",
    "df[\"label\"] = YTest\n",
    "\n",
    "type0 = df[df['label'].isin([0])]\n",
    "type1 = df[df['label'].isin([1])]\n",
    "\n",
    "df_type0 = pd.DataFrame()\n",
    "df_type0[\"feature6\"] = type0[\"feature6\"]\n",
    "df_type0[\"feature7\"] = type0[\"feature7\"]\n",
    "df_type0[\"feature1\"] = type0[\"feature1\"]\n",
    "df_type0[\"feature2\"] = type0[\"feature2\"]\n",
    "df_type0[\"label\"]= type0[\"label\"]\n",
    "\n",
    "df_type1 = pd.DataFrame()\n",
    "df_type1[\"feature6\"] = type1[\"feature6\"]\n",
    "df_type1[\"feature7\"] = type1[\"feature7\"]\n",
    "df_type1[\"feature1\"] = type1[\"feature1\"]\n",
    "df_type1[\"feature2\"] = type1[\"feature2\"]\n",
    "df_type1[\"label\"]= type1[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb535038",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax1 = fig.add_subplot(2,2,1,title='the Scatterplot of feature 6 and feature 7',xlabel='Feature 6',ylabel='Feature 7')\n",
    "ax2 = fig.add_subplot(2,2,2,title='the Scatterplot of feature 1 and feature 2',xlabel='Feature 1',ylabel='Feature 2')\n",
    "ax1.scatter(data=df_type0,x='feature6',y='feature7',color='mediumturquoise',s=3,label='0')\n",
    "ax1.scatter(data=df_type1,x='feature6',y='feature7',color='crimson',s=3,label='1')\n",
    "ax1.scatter(x=293.73, y=131.61,s=15,marker='x',c='black',label='Centroid 1')\n",
    "ax1.scatter(x=1003.72, y=632.81,s=15,marker='D',c='black',label='Centroid 2')\n",
    "ax2.scatter(data=df_type0,x='feature1',y='feature2',color='mediumturquoise',s=3,label='0')\n",
    "ax2.scatter(data=df_type1,x='feature1',y='feature2',color='crimson',s=3,label='1')\n",
    "ax2.scatter(x=49.38, y=791.59,s=15,marker='x',c='black',label='Centroid 1')\n",
    "ax2.scatter(x=13.73, y=170.94,s=15,marker='D',c='black',label='Centroid 2')\n",
    "\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper left')\n",
    "fig.subplots_adjust(wspace=1)\n",
    "fig.tight_layout(pad=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ceffef",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size=3>1) The description of software used: </br>At first, two random points are chosen as the centroids, and the points are assigned to two centroids (two clusters formed). Then, the distances between points and centroids are calculated by iterating centroids and data assignment, until the sum of distances is to a minimum. In the end, the optimized centroids are found and the iteration stops.</br></br>2) Values of two cluster centers: </br>The first cluster center: [5.69, 49.38, 791.59, 3847.71, 3385.89, 1359.89, 293.73, 131.61, 70.73, 39.64, 19.44, 4.24, 0.44]</br>The second cluster center: [2.19, 13.73, 170.94, 1394.36, 3188.53, 2624.62, 1003.72, 632.81, 495.83, 295.4, 145.8, 29.2, 2.84]</br></br>3) Two plots: The plots show different clusters of Test data (the blue and red \"clusters\" are defined by the labels, not by the algorithm). In the left plot, the two clusters have distinct characteristics. But in the right plot, two clusters are overlapped more.</br>\n",
    "p.s. the centroids are plotted just for verifying. In the two plots, we could see the location of the centroids make sense (to some extent). So, the k-means algorithm works well here.</br></br> </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IDS)",
   "language": "python",
   "name": "ids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
